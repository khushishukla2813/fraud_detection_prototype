{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\Lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\ANACONDA\\Lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "D:\\ANACONDA\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "D:\\ANACONDA\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_4996\\3086887181.py:19: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\ANACONDA\\Lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.cm as cm\n",
    "from scipy import ndimage\n",
    "from skimage.measure import regionprops\n",
    "from skimage import io\n",
    "from skimage.filters import threshold_otsu   # For finding the threshold for grayscale to binary conversion\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "import keras\n",
    "from tensorflow.python.framework import ops\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "genuine_image_paths = r\"D:\\ML_Practice\\Signature-Forgery-Detection\\real\"\n",
    "forged_image_paths = r\"D:\\ML_Practice\\Signature-Forgery-Detection\\forged\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rgbgrey(img):\n",
    "    # Converts rgb to grayscale\n",
    "    greyimg = np.zeros((img.shape[0], img.shape[1]))\n",
    "    for row in range(len(img)):\n",
    "        for col in range(len(img[row])):\n",
    "            greyimg[row][col] = np.average(img[row][col])\n",
    "    return greyimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def greybin(img):\n",
    "    # Converts grayscale to binary\n",
    "    blur_radius = 0.8\n",
    "    img = ndimage.gaussian_filter(img, blur_radius)  # to remove small components or noise\n",
    "#     img = ndimage.binary_erosion(img).astype(img.dtype)\n",
    "    thres = threshold_otsu(img)\n",
    "    binimg = img > thres\n",
    "    binimg = np.logical_not(binimg)\n",
    "    return binimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preproc(path, img=None, display=True):\n",
    "    if img is None:\n",
    "        img = mpimg.imread(path)\n",
    "    if display:\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "    grey = rgbgrey(img) #rgb to grey\n",
    "    if display:\n",
    "        plt.imshow(grey, cmap = matplotlib.cm.Greys_r)\n",
    "        plt.show()\n",
    "    binimg = greybin(grey) #grey to binary\n",
    "    if display:\n",
    "        plt.imshow(binimg, cmap = matplotlib.cm.Greys_r)\n",
    "        plt.show()\n",
    "    r, c = np.where(binimg==1)\n",
    "    # Now we will make a bounding box with the boundary as the position of pixels on extreme.\n",
    "    # Thus we will get a cropped image with only the signature part.\n",
    "    signimg = binimg[r.min(): r.max(), c.min(): c.max()]\n",
    "    if display:\n",
    "        plt.imshow(signimg, cmap = matplotlib.cm.Greys_r)\n",
    "        plt.show()\n",
    "    return signimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Ratio(img):\n",
    "    a = 0\n",
    "    for row in range(len(img)):\n",
    "        for col in range(len(img[0])):\n",
    "            if img[row][col]==True:\n",
    "                a = a+1\n",
    "    total = img.shape[0] * img.shape[1]\n",
    "    return a/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Centroid(img):\n",
    "    numOfWhites = 0\n",
    "    a = np.array([0,0])\n",
    "    for row in range(len(img)):\n",
    "        for col in range(len(img[0])):\n",
    "            if img[row][col]==True:\n",
    "                b = np.array([row,col])\n",
    "                a = np.add(a,b)\n",
    "                numOfWhites += 1\n",
    "    rowcols = np.array([img.shape[0], img.shape[1]])\n",
    "    centroid = a/numOfWhites\n",
    "    centroid = centroid/rowcols\n",
    "    return centroid[0], centroid[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def EccentricitySolidity(img):\n",
    "    r = regionprops(img.astype(\"int8\"))\n",
    "    return r[0].eccentricity, r[0].solidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SkewKurtosis(img):\n",
    "    h,w = img.shape\n",
    "    x = range(w)  # cols value\n",
    "    y = range(h)  # rows value\n",
    "    #calculate projections along the x and y axes\n",
    "    xp = np.sum(img,axis=0)\n",
    "    yp = np.sum(img,axis=1)\n",
    "    #centroid\n",
    "    cx = np.sum(x*xp)/np.sum(xp)\n",
    "    cy = np.sum(y*yp)/np.sum(yp)\n",
    "    #standard deviation\n",
    "    x2 = (x-cx)**2\n",
    "    y2 = (y-cy)**2\n",
    "    sx = np.sqrt(np.sum(x2*xp)/np.sum(img))\n",
    "    sy = np.sqrt(np.sum(y2*yp)/np.sum(img))\n",
    "    \n",
    "    #skewness\n",
    "    x3 = (x-cx)**3\n",
    "    y3 = (y-cy)**3\n",
    "    skewx = np.sum(xp*x3)/(np.sum(img) * sx**3)\n",
    "    skewy = np.sum(yp*y3)/(np.sum(img) * sy**3)\n",
    "\n",
    "    #Kurtosis\n",
    "    x4 = (x-cx)**4\n",
    "    y4 = (y-cy)**4\n",
    "    # 3 is subtracted to calculate relative to the normal distribution\n",
    "    kurtx = np.sum(xp*x4)/(np.sum(img) * sx**4) - 3\n",
    "    kurty = np.sum(yp*y4)/(np.sum(img) * sy**4) - 3\n",
    "\n",
    "    return (skewx , skewy), (kurtx, kurty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getFeatures(path, img=None, display=False):\n",
    "    if img is None:\n",
    "        img = mpimg.imread(path)\n",
    "    img = preproc(path, display=display)\n",
    "    ratio = Ratio(img)\n",
    "    centroid = Centroid(img)\n",
    "    eccentricity, solidity = EccentricitySolidity(img)\n",
    "    skewness, kurtosis = SkewKurtosis(img)\n",
    "    retVal = (ratio, centroid, eccentricity, solidity, skewness, kurtosis)\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getCSVFeatures(path, img=None, display=False):\n",
    "    if img is None:\n",
    "        img = mpimg.imread(path)\n",
    "    temp = getFeatures(path, display=display)\n",
    "    features = (temp[0], temp[1][0], temp[1][1], temp[2], temp[3], temp[4][0], temp[4][1], temp[5][0], temp[5][1])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def makeCSV():\n",
    "    if not(os.path.exists('D:\\\\ML_Practice\\\\Signature-Forgery-Detection\\\\Features')):\n",
    "        os.mkdir('D:\\\\ML_Practice\\\\Signature-Forgery-Detection\\\\Features')\n",
    "        print('New folder \"Features\" created')\n",
    "    if not(os.path.exists('D:\\\\ML_Practice\\\\Signature-Forgery-Detection\\\\Features/Training')):\n",
    "        os.mkdir('D:\\\\ML_Practice\\\\Signature-Forgery-Detection\\\\Features/Training')\n",
    "        print('New folder \"Features/Training\" created')\n",
    "    if not(os.path.exists('D:\\\\ML_Practice\\\\Signature-Forgery-Detection\\\\Features/Testing')):\n",
    "        os.mkdir('D:\\\\ML_Practice\\\\Signature-Forgery-Detection\\\\Features/Testing')\n",
    "        print('New folder \"Features/Testing\" created')\n",
    "    # genuine signatures path\n",
    "    gpath = genuine_image_paths\n",
    "    # forged signatures path\n",
    "    fpath = forged_image_paths\n",
    "    for person in range(1,13):\n",
    "        per = ('00'+str(person))[-3:]\n",
    "        print('Saving features for person id-',per)\n",
    "        \n",
    "        with open('D:\\\\ML_Practice\\\\Signature-Forgery-Detection\\\\Features\\\\Training/training_'+per+'.csv', 'w') as handle:\n",
    "            handle.write('ratio,cent_y,cent_x,eccentricity,solidity,skew_x,skew_y,kurt_x,kurt_y,output\\n')\n",
    "            # Training set\n",
    "            for i in range(0,3):\n",
    "                source = os.path.join(gpath, per+per+'_00'+str(i)+'.png')\n",
    "                features = getCSVFeatures(path=source)\n",
    "                handle.write(','.join(map(str, features))+',1\\n')\n",
    "            for i in range(0,3):\n",
    "                source = os.path.join(fpath, '021'+per+'_00'+str(i)+'.png')\n",
    "                features = getCSVFeatures(path=source)\n",
    "                handle.write(','.join(map(str, features))+',0\\n')\n",
    "        \n",
    "        with open('D:\\\\ML_Practice\\\\Signature-Forgery-Detection\\\\Features\\\\Testing/testing_'+per+'.csv', 'w') as handle:\n",
    "            handle.write('ratio,cent_y,cent_x,eccentricity,solidity,skew_x,skew_y,kurt_x,kurt_y,output\\n')\n",
    "            # Testing set\n",
    "            for i in range(3, 5):\n",
    "                source = os.path.join(gpath, per+per+'_00'+str(i)+'.png')\n",
    "                features = getCSVFeatures(path=source)\n",
    "                handle.write(','.join(map(str, features))+',1\\n')\n",
    "            for i in range(3,5):\n",
    "                source = os.path.join(fpath, '021'+per+'_00'+str(i)+'.png')\n",
    "                features = getCSVFeatures(path=source)\n",
    "                handle.write(','.join(map(str, features))+',0\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving features for person id- 001\n",
      "Saving features for person id- 002\n",
      "Saving features for person id- 003\n",
      "Saving features for person id- 004\n",
      "Saving features for person id- 005\n",
      "Saving features for person id- 006\n",
      "Saving features for person id- 007\n",
      "Saving features for person id- 008\n",
      "Saving features for person id- 009\n",
      "Saving features for person id- 010\n",
      "Saving features for person id- 011\n",
      "Saving features for person id- 012\n"
     ]
    }
   ],
   "source": [
    "makeCSV()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def testing(path):\n",
    "    feature = getCSVFeatures(path)\n",
    "    if not(os.path.exists('D:\\\\ML_Practice\\\\Signature-Forgery-Detection/TestFeatures')):\n",
    "        os.mkdir('D:\\\\ML_Practice\\\\Signature-Forgery-Detection/TestFeatures')\n",
    "    with open('D:\\\\ML_Practice\\\\Signature-Forgery-Detection\\\\TestFeatures/testcsv.csv', 'w') as handle:\n",
    "        handle.write('ratio,cent_y,cent_x,eccentricity,solidity,skew_x,skew_y,kurt_x,kurt_y\\n')\n",
    "        handle.write(','.join(map(str, feature))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter person's id :  009\n",
      "Enter path of signature image :  D:\\ML_Practice\\Signature-Forgery-Detection\\forged\\021004_001.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_4996\\1560142516.py:76: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "Forged Image\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_input = 9\n",
    "train_person_id = input(\"Enter person's id : \")\n",
    "test_image_path = input(\"Enter path of signature image : \")\n",
    "train_path = 'D:\\\\ML_Practice\\\\Signature-Forgery-Detection\\\\Features\\\\Training/training_'+train_person_id+'.csv'\n",
    "testing(test_image_path)\n",
    "test_path = 'TestFeatures/testcsv.csv'\n",
    "\n",
    "def readCSV(train_path, test_path, type2=False):\n",
    "    # Reading train data\n",
    "    df = pd.read_csv(train_path, usecols=range(n_input))\n",
    "    train_input = np.array(df.values)\n",
    "    train_input = train_input.astype(np.float32, copy=False)  # Converting input to float_32\n",
    "    df = pd.read_csv(train_path, usecols=(n_input,))\n",
    "    temp = [elem[0] for elem in df.values]\n",
    "    correct = np.array(temp)\n",
    "    corr_train = keras.utils.to_categorical(correct,2)      # Converting to one hot\n",
    "    # Reading test data\n",
    "    df = pd.read_csv(test_path, usecols=range(n_input))\n",
    "    test_input = np.array(df.values)\n",
    "    test_input = test_input.astype(np.float32, copy=False)\n",
    "    if not(type2):\n",
    "        df = pd.read_csv(test_path, usecols=(n_input,))\n",
    "        temp = [elem[0] for elem in df.values]\n",
    "        correct = np.array(temp)\n",
    "        corr_test = kearas.utils.to_categorical(correct,2)      # Converting to one hot\n",
    "    if not(type2):\n",
    "        return train_input, corr_train, test_input, corr_test\n",
    "    else:\n",
    "        return train_input, corr_train, test_input\n",
    "\n",
    "ops.reset_default_graph()\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 1000\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 7 # 1st layer number of neurons\n",
    "n_hidden_2 = 10 # 2nd layer number of neurons\n",
    "n_hidden_3 = 30 # 3rd layer\n",
    "n_classes = 2 # no. of classes (genuine or forged)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], seed=1)),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes], seed=2))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1], seed=3)),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes], seed=4))\n",
    "}\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x):\n",
    "    layer_1 = tf.tanh((tf.matmul(x, weights['h1']) + biases['b1']))\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    out_layer = tf.tanh(tf.matmul(layer_1, weights['out']) + biases['out'])\n",
    "    return out_layer\n",
    "\n",
    "# Construct model\n",
    "logits = multilayer_perceptron(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.squared_difference(logits, Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# For accuracies\n",
    "pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "correct_prediction = tf.equal(tf.argmax(pred,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "def evaluate(train_path, test_path, type2=False):   \n",
    "    if not(type2):\n",
    "        train_input, corr_train, test_input, corr_test = readCSV(train_path, test_path)\n",
    "    else:\n",
    "        train_input, corr_train, test_input = readCSV(train_path, test_path, type2)\n",
    "    ans = 'Random'\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        # Training cycle\n",
    "        for epoch in range(training_epochs):\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, cost = sess.run([train_op, loss_op], feed_dict={X: train_input, Y: corr_train})\n",
    "            if cost<0.0001:\n",
    "                break\n",
    "#             # Display logs per epoch step\n",
    "#             if epoch % 999 == 0:\n",
    "#                 print(\"Epoch:\", '%04d' % (epoch+1), \"cost={:.9f}\".format(cost))\n",
    "#         print(\"Optimization Finished!\")\n",
    "        \n",
    "        # Finding accuracies\n",
    "        accuracy1 =  accuracy.eval({X: train_input, Y: corr_train})\n",
    "#         print(\"Accuracy for train:\", accuracy1)\n",
    "#         print(\"Accuracy for test:\", accuracy2)\n",
    "        if type2 is False:\n",
    "            accuracy2 =  accuracy.eval({X: test_input, Y: corr_test})\n",
    "            return accuracy1, accuracy2\n",
    "        else:\n",
    "            prediction = pred.eval({X: test_input})\n",
    "            if prediction[0][1]>prediction[0][0]:\n",
    "                print('Genuine Image')\n",
    "                return True\n",
    "            else:\n",
    "                print('Forged Image')\n",
    "                return False\n",
    "\n",
    "\n",
    "def trainAndTest(rate=0.001, epochs=1700, neurons=7, display=False):    \n",
    "    start = time()\n",
    "\n",
    "    # Parameters\n",
    "    global training_rate, training_epochs, n_hidden_1\n",
    "    learning_rate = rate\n",
    "    training_epochs = epochs\n",
    "\n",
    "    # Network Parameters\n",
    "    n_hidden_1 = neurons # 1st layer number of neurons\n",
    "    n_hidden_2 = 7 # 2nd layer number of neurons\n",
    "    n_hidden_3 = 30 # 3rd layer\n",
    "\n",
    "    train_avg, test_avg = 0, 0\n",
    "    n = 10\n",
    "    for i in range(1,n+1):\n",
    "        if display:\n",
    "            print(\"Running for Person id\",i)\n",
    "        temp = ('0'+str(i))[-2:]\n",
    "        train_score, test_score = evaluate(train_path.replace('01',temp), test_path.replace('01',temp))\n",
    "        train_avg += train_score\n",
    "        test_avg += test_score\n",
    "    if display:\n",
    "#         print(\"Number of neurons in Hidden layer-\", n_hidden_1)\n",
    "        print(\"Training average-\", train_avg/n)\n",
    "        print(\"Testing average-\", test_avg/n)\n",
    "        print(\"Time taken-\", time()-start)\n",
    "    return train_avg/n, test_avg/n, (time()-start)/n\n",
    "\n",
    "\n",
    "evaluate(train_path, test_path, type2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving features for person id - 001\n",
      "Saving features for person id - 002\n",
      "Saving features for person id - 003\n",
      "Saving features for person id - 004\n",
      "Saving features for person id - 005\n",
      "Saving features for person id - 006\n",
      "Saving features for person id - 007\n",
      "Saving features for person id - 008\n",
      "Saving features for person id - 009\n",
      "Saving features for person id - 010\n",
      "Saving features for person id - 011\n",
      "Saving features for person id - 012\n",
      "Training Complete!\n",
      "Model saved to signature_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pickle\n",
    "from time import time\n",
    "\n",
    "# Ensure TensorFlow uses a compatible version\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "ops = tf.compat.v1\n",
    "\n",
    "# Define number of inputs\n",
    "n_input = 9\n",
    "\n",
    "# Function to create necessary directories\n",
    "def makeCSV():\n",
    "    base_dir = 'D:\\\\ML_Practice\\\\Signature-Forgery-Detection\\\\Features'\n",
    "    \n",
    "    for sub_dir in [\"\", \"Training\", \"Testing\"]:\n",
    "        full_path = os.path.join(base_dir, sub_dir)\n",
    "        if not os.path.exists(full_path):\n",
    "            os.mkdir(full_path)\n",
    "            print(f'New folder \"{full_path}\" created')\n",
    "\n",
    "    gpath = genuine_image_paths  # Define these paths correctly\n",
    "    fpath = forged_image_paths  \n",
    "\n",
    "    for person in range(1, 13):\n",
    "        per = f\"{person:03d}\"\n",
    "        print(f'Saving features for person id - {per}')\n",
    "        \n",
    "        train_file = os.path.join(base_dir, 'Training', f'training_{per}.csv')\n",
    "        test_file = os.path.join(base_dir, 'Testing', f'testing_{per}.csv')\n",
    "\n",
    "        with open(train_file, 'w') as handle:\n",
    "            handle.write('ratio,cent_y,cent_x,eccentricity,solidity,skew_x,skew_y,kurt_x,kurt_y,output\\n')\n",
    "            for i in range(0, 3):\n",
    "                source = os.path.join(gpath, f'{per}{per}_00{i}.png')\n",
    "                features = getCSVFeatures(path=source)\n",
    "                handle.write(','.join(map(str, features)) + ',1\\n')\n",
    "\n",
    "            for i in range(0, 3):\n",
    "                source = os.path.join(fpath, f'021{per}_00{i}.png')\n",
    "                features = getCSVFeatures(path=source)\n",
    "                handle.write(','.join(map(str, features)) + ',0\\n')\n",
    "\n",
    "        with open(test_file, 'w') as handle:\n",
    "            handle.write('ratio,cent_y,cent_x,eccentricity,solidity,skew_x,skew_y,kurt_x,kurt_y,output\\n')\n",
    "            for i in range(3, 5):\n",
    "                source = os.path.join(gpath, f'{per}{per}_00{i}.png')\n",
    "                features = getCSVFeatures(path=source)\n",
    "                handle.write(','.join(map(str, features)) + ',1\\n')\n",
    "\n",
    "            for i in range(3, 5):\n",
    "                source = os.path.join(fpath, f'021{per}_00{i}.png')\n",
    "                features = getCSVFeatures(path=source)\n",
    "                handle.write(','.join(map(str, features)) + ',0\\n')\n",
    "\n",
    "# Execute CSV creation\n",
    "makeCSV()\n",
    "\n",
    "# Function to read CSV files correctly\n",
    "def readCSV(train_path, test_path, type2=False):\n",
    "    try:\n",
    "        df_train = pd.read_csv(train_path)\n",
    "        if df_train.shape[1] < n_input + 1:\n",
    "            raise ValueError(f\"Expected at least {n_input + 1} columns in {train_path}, but found {df_train.shape[1]}\")\n",
    "\n",
    "        train_input = np.array(df_train.iloc[:, :n_input].values, dtype=np.float32)\n",
    "        correct_train = np.array(df_train.iloc[:, n_input].values, dtype=np.int32)\n",
    "        corr_train = keras.utils.to_categorical(correct_train, 2)\n",
    "\n",
    "        df_test = pd.read_csv(test_path)\n",
    "        if df_test.shape[1] < n_input:\n",
    "            raise ValueError(f\"Expected at least {n_input} columns in {test_path}, but found {df_test.shape[1]}\")\n",
    "\n",
    "        test_input = np.array(df_test.iloc[:, :n_input].values, dtype=np.float32)\n",
    "\n",
    "        if not type2:\n",
    "            correct_test = np.array(df_test.iloc[:, n_input].values, dtype=np.int32)\n",
    "            corr_test = keras.utils.to_categorical(correct_test, 2)\n",
    "            return train_input, corr_train, test_input, corr_test\n",
    "        else:\n",
    "            return train_input, corr_train, test_input\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error reading CSV:\", e)\n",
    "        return None, None, None, None\n",
    "\n",
    "# Neural Network Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 1000\n",
    "n_hidden_1 = 7\n",
    "n_hidden_2 = 10\n",
    "n_hidden_3 = 30\n",
    "n_classes = 2\n",
    "\n",
    "# TensorFlow placeholders\n",
    "X = tf.compat.v1.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.compat.v1.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Network weights and biases\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random.normal([n_input, n_hidden_1], seed=1)),\n",
    "    'h2': tf.Variable(tf.random.normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random.normal([n_hidden_2, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random.normal([n_hidden_1, n_classes], seed=2))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random.normal([n_hidden_1], seed=3)),\n",
    "    'b2': tf.Variable(tf.random.normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random.normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random.normal([n_classes], seed=4))\n",
    "}\n",
    "\n",
    "# MLP Model\n",
    "def multilayer_perceptron(x):\n",
    "    layer_1 = tf.nn.tanh(tf.matmul(x, weights['h1']) + biases['b1'])\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    out_layer = tf.nn.tanh(tf.matmul(layer_1, weights['out']) + biases['out'])\n",
    "    return out_layer\n",
    "\n",
    "# Define loss and optimizer\n",
    "logits = multilayer_perceptron(X)\n",
    "#loss_op = tf.reduce_mean(tf.squared_difference(logits, Y))\n",
    "loss_op = tf.reduce_mean(tf.math.squared_difference(logits, Y))\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Accuracy computation\n",
    "pred = tf.nn.softmax(logits)\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "# Training Function\n",
    "def train_and_save_model(train_path, test_path, model_filename='signature_model.pkl'):\n",
    "    train_input, corr_train, test_input, corr_test = readCSV(train_path, test_path)\n",
    "\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(training_epochs):\n",
    "            _, cost = sess.run([train_op, loss_op], feed_dict={X: train_input, Y: corr_train})\n",
    "            if cost < 0.0001:\n",
    "                break\n",
    "\n",
    "        print(\"Training Complete!\")\n",
    "        \n",
    "        # Save model to pickle\n",
    "        model_data = {\n",
    "            'weights': sess.run(weights),\n",
    "            'biases': sess.run(biases)\n",
    "        }\n",
    "        with open(model_filename, 'wb') as file:\n",
    "            pickle.dump(model_data, file)\n",
    "        print(f\"Model saved to {model_filename}\")\n",
    "\n",
    "# Execute training and save model\n",
    "train_path = 'D:\\\\ML_Practice\\\\Signature-Forgery-Detection\\\\Features\\\\Training/training_001.csv'\n",
    "test_path = 'D:\\\\ML_Practice\\\\Signature-Forgery-Detection\\\\Features\\\\Testing/testing_001.csv'\n",
    "train_and_save_model(train_path, test_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📌 CELL 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# import os\n",
    "# import joblib  # For saving and loading model\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 📌 CELL 2: Feature Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def getCSVFeatures(path):\n",
    "#     # Dummy function to extract features from an image (Replace with actual feature extraction)\n",
    "#     # This function should return an array of 9 extracted features\n",
    "#     return np.random.rand(9)  # Replace with real feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 📌 CELL 3: Create CSV Files for Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def makeCSV():\n",
    "#     base_path = 'D:\\\\ML_Practice\\\\Signature-Forgery-Detection\\\\Features1'\n",
    "    \n",
    "#     # Create directories if they don't exist\n",
    "#     for folder in ['Features1', 'Features1/Training', 'Features1/Testing']:\n",
    "#         path = os.path.join('D:\\\\ML_Practice\\\\Signature-Forgery-Detection', folder)\n",
    "#         if not os.path.exists(path):\n",
    "#             os.mkdir(path)\n",
    "#             print(f'📁 Created new folder: \"{folder}\"')\n",
    "\n",
    "#     # Paths for genuine and forged images\n",
    "#     gpath = 'D:\\\\ML_Practice\\\\Signature-Forgery-Detection\\\\genuine'\n",
    "#     fpath = 'D:\\\\ML_Practice\\\\Signature-Forgery-Detection\\\\forged'\n",
    "\n",
    "#     print(\"\\n🔄 Starting feature extraction and saving CSV files...\\n\")\n",
    "\n",
    "#     for person in range(1, 13):  # Loop for persons 001 to 012\n",
    "#         per = f'{person:03d}'  # Format person ID (001, 002, etc.)\n",
    "#         print(f'✅ Saving features for person id - {per}')\n",
    "\n",
    "#         # Training CSV\n",
    "#         train_file = f'{base_path}/Training/training_{per}.csv'\n",
    "#         with open(train_file, 'w') as handle:\n",
    "#             handle.write('ratio,cent_y,cent_x,eccentricity,solidity,skew_x,skew_y,kurt_x,kurt_y,output\\n')\n",
    "#             for i in range(3):  # 3 genuine + 3 forged\n",
    "#                 handle.write(','.join(map(str, getCSVFeatures(f'{gpath}/{per}{per}_00{i}.png'))) + ',1\\n')\n",
    "#                 handle.write(','.join(map(str, getCSVFeatures(f'{fpath}/021{per}_00{i}.png'))) + ',0\\n')\n",
    "\n",
    "#         # Testing CSV\n",
    "#         test_file = f'{base_path}/Testing/testing_{per}.csv'\n",
    "#         with open(test_file, 'w') as handle:\n",
    "#             handle.write('ratio,cent_y,cent_x,eccentricity,solidity,skew_x,skew_y,kurt_x,kurt_y,output\\n')\n",
    "#             for i in range(3, 5):  # 2 genuine + 2 forged\n",
    "#                 handle.write(','.join(map(str, getCSVFeatures(f'{gpath}/{per}{per}_00{i}.png'))) + ',1\\n')\n",
    "#                 handle.write(','.join(map(str, getCSVFeatures(f'{fpath}/021{per}_00{i}.png'))) + ',0\\n')\n",
    "\n",
    "#     print(\"\\n✅ Feature extraction complete! All CSV files saved successfully. 🎯\")\n",
    "\n",
    "# # Run the function\n",
    "# makeCSV()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## 📌 CELL 5: Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def testing(path):\n",
    "#     feature = getCSVFeatures(path)\n",
    "\n",
    "#     # Create folder if not exists\n",
    "#     test_folder = 'D:\\\\ML_Practice\\\\Signature-Forgery-Detection\\\\TestFeatures1'\n",
    "#     if not os.path.exists(test_folder):\n",
    "#         os.mkdir(test_folder)\n",
    "\n",
    "#     # Save extracted features\n",
    "#     with open(f'{test_folder}/testcsv.csv', 'w') as handle:\n",
    "#         handle.write('ratio,cent_y,cent_x,eccentricity,solidity,skew_x,skew_y,kurt_x,kurt_y\\n')\n",
    "#         handle.write(','.join(map(str, feature)) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📌 CELL 6: Read CSV and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # import pandas as pd\n",
    "# # import numpy as np\n",
    "# # from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# # n_input = 9  # Number of feature columns\n",
    "\n",
    "# # def readCSV(train_path, test_path, type2=False):\n",
    "# #     # Check the actual number of columns in the CSV before reading\n",
    "# #     df_sample = pd.read_csv(train_path, nrows=1)  # Read only the first row\n",
    "# #     actual_columns = df_sample.shape[1]  # Get actual column count\n",
    "\n",
    "# #     if actual_columns < n_input + 1:  # +1 for the label column\n",
    "# #         raise ValueError(f\"Expected at least {n_input + 1} columns, but found only {actual_columns} in {train_path}\")\n",
    "\n",
    "# #     # Read feature columns\n",
    "# #     df_train = pd.read_csv(train_path, usecols=range(n_input), header=0)\n",
    "# #     train_input = np.array(df_train.values, dtype=np.float32)\n",
    "\n",
    "# #     # Read label column\n",
    "# #     df_labels = pd.read_csv(train_path, usecols=[n_input], header=0)\n",
    "# #     correct = np.array(df_labels.values).flatten()\n",
    "# #     corr_train = to_categorical(correct, num_classes=2)  # Convert labels to one-hot encoding\n",
    "\n",
    "# #     # Read test data\n",
    "# #     df_test = pd.read_csv(test_path, usecols=range(n_input), header=None)\n",
    "# #     test_input = np.array(df_test.values, dtype=np.float32)\n",
    "\n",
    "# #     if not type2:\n",
    "# #         df_test_labels = pd.read_csv(test_path, usecols=[n_input], header=None)\n",
    "# #         correct = np.array(df_test_labels.values).flatten()\n",
    "# #         corr_test = to_categorical(correct, num_classes=2)\n",
    "# #         return train_input, corr_train, test_input, corr_test\n",
    "\n",
    "# #     return train_input, corr_train, test_input\n",
    "\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# n_input = 9  # Number of feature columns\n",
    "\n",
    "# def readCSV(train_path, test_path, type2=False):\n",
    "#     # Read training data\n",
    "#     df_train = pd.read_csv(train_path, header=0)  \n",
    "#     df_train = df_train.apply(pd.to_numeric, errors='coerce')  # Convert non-numeric to NaN\n",
    "\n",
    "#     # Extract features and labels\n",
    "#     train_input = np.array(df_train.iloc[:, :n_input], dtype=np.float32)\n",
    "#     correct = np.array(df_train.iloc[:, n_input], dtype=np.int32)\n",
    "#     corr_train = to_categorical(correct, num_classes=2)\n",
    "\n",
    "#     # Read test data\n",
    "#     df_test = pd.read_csv(test_path, header=0)\n",
    "#     df_test = df_test.apply(pd.to_numeric, errors='coerce')\n",
    "#     test_input = np.array(df_test.iloc[:, :n_input], dtype=np.float32)\n",
    "\n",
    "#     # Handle test labels properly\n",
    "#     if not type2:\n",
    "#         correct = np.array(df_test.iloc[:, n_input], dtype=np.int32)\n",
    "#         corr_test = to_categorical(correct, num_classes=2)\n",
    "#     else:\n",
    "#         corr_test = None  # Return None if type2=True to avoid unpacking errors\n",
    "\n",
    "#     return train_input, corr_train, test_input, corr_test  # Always return 4 values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## 📌 CELL 7: Define Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # Parameters\n",
    "# learning_rate = 0.001\n",
    "# training_epochs = 1000\n",
    "# n_hidden_1 = 7\n",
    "# n_hidden_2 = 10\n",
    "# n_hidden_3 = 30\n",
    "# n_classes = 2\n",
    "# n_input = 9  # Define input feature size\n",
    "\n",
    "# # Define Model Using Functional API\n",
    "# inputs = tf.keras.Input(shape=(n_input,))\n",
    "# x = tf.keras.layers.Dense(n_hidden_1, activation='tanh')(inputs)\n",
    "# x = tf.keras.layers.Dense(n_hidden_2, activation='relu')(x)\n",
    "# x = tf.keras.layers.Dense(n_hidden_3, activation='relu')(x)\n",
    "# outputs = tf.keras.layers.Dense(n_classes, activation='tanh')(x)\n",
    "\n",
    "# # Build Model\n",
    "# mlp_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# # Show Model Summary\n",
    "# mlp_model.summary()\n",
    "\n",
    "# # Compile Model (Adding Loss & Optimizer)\n",
    "# mlp_model.compile(\n",
    "#     optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "#     loss=tf.keras.losses.MeanSquaredError(),\n",
    "#     metrics=[\"accuracy\"]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## 📌 CELL 8: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# def evaluate(train_path, test_path, type2=False):\n",
    "#     train_input, corr_train, test_input, corr_test = readCSV(train_path, test_path) if not type2 else readCSV(train_path, test_path, type2)\n",
    "\n",
    "#     init = tf.compat.v1.global_variables_initializer()  # Initialize variables\n",
    "\n",
    "#     with tf.compat.v1.Session() as sess:\n",
    "#         sess.run(init)\n",
    "#         for epoch in range(training_epochs):\n",
    "#             _, cost = sess.run([train_op, loss_op], feed_dict={X: train_input, Y: corr_train})\n",
    "\n",
    "#         if type2:\n",
    "#             prediction = sess.run(pred_op, feed_dict={X: test_input})\n",
    "#             return prediction.argmax(axis=1) == 1  # Assuming 1 = Genuine, 0 = Forged\n",
    "#         else:\n",
    "#             accuracy = sess.run(accuracy_op, feed_dict={X: test_input, Y: corr_test})\n",
    "#             return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## 📌 CELL 9: Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def predict_signature():\n",
    "#     person_id = input(\"Enter person's ID: \")\n",
    "#     test_image_path = input(\"Enter the path of the signature image: \")\n",
    "\n",
    "#     train_path = f'D:\\\\ML_Practice\\\\Signature-Forgery-Detection\\\\Features1\\\\Training/training_{person_id}.csv'\n",
    "#     testing(test_image_path)\n",
    "#     test_path = 'D:\\\\ML_Practice\\\\Signature-Forgery-Detection\\\\TestFeatures1/testcsv.csv'\n",
    "\n",
    "#     accuracy_test = evaluate(train_path, test_path, type2=True)\n",
    "#     print(\"\\n✅ Prediction: The signature is **Genuine**\" if accuracy_test else \"\\n❌ Prediction: The signature is **Forged**\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## 📌 CELL 10: Run Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict_signature()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## 📌 CELL: Save Model as .pkl File for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "\n",
    "# # Load training data\n",
    "# train_person_id = \"001\"  # Change this based on person ID\n",
    "# train_path = f'D:\\\\ML_Practice\\\\Signature-Forgery-Detection\\\\Features\\\\Training/training_{train_person_id}.csv'\n",
    "\n",
    "# # Read CSV\n",
    "# df = pd.read_csv(train_path)\n",
    "# X_train = df.iloc[:, :-1].values  # All columns except last (features)\n",
    "# y_train = df.iloc[:, -1].values   # Last column (labels)\n",
    "\n",
    "# # Convert labels to categorical (one-hot encoding)\n",
    "# y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
    "\n",
    "# # Define Model\n",
    "# model = Sequential([\n",
    "#     Dense(7, activation='tanh', input_shape=(X_train.shape[1],)),\n",
    "#     Dense(10, activation='relu'),\n",
    "#     Dense(30, activation='relu'),\n",
    "#     Dense(2, activation='softmax')  # Output layer with 2 classes\n",
    "# ])\n",
    "\n",
    "# # Compile Model\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Train Model\n",
    "# model.fit(X_train, y_train, epochs=100, verbose=1)\n",
    "\n",
    "# # Save Model as .pkl\n",
    "# pkl_filename = \"D:\\\\ML_Practice\\\\Signature-Forgery-Detection\\\\model.pkl\"\n",
    "# joblib.dump(model, pkl_filename)\n",
    "\n",
    "# print(f\"✅ Model saved successfully at: {pkl_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
